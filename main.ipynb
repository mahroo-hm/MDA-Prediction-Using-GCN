{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "69a8d350-f869-471e-975e-f9ab5c95e9ba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import csv\n",
    "from numpy import genfromtxt, interp\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy import interpolate\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import dgl.function as fn\n",
    "import torch.nn.functional as F\n",
    "from torch import tensor\n",
    "import torch_geometric.utils\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "import dgl\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8ccea2bf-32f1-4faf-8b22-7151fa7072cf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def nSGCConv(graph, feats, order):\n",
    "    with graph.local_scope():\n",
    "        degs = graph.in_degrees().float().clamp(min=1)\n",
    "        norm = torch.pow(degs, -0.5).to(feats.device).unsqueeze(1)\n",
    "        graph.ndata['norm'] = norm\n",
    "        graph.apply_edges(fn.u_mul_v('norm', 'norm', 'weight'))\n",
    "        x = feats\n",
    "        # x = F.dropout(feats, p=0.5)\n",
    "        y = 0 + feats\n",
    "        for i in range(order):\n",
    "            graph.ndata['h'] = x\n",
    "            graph.update_all(fn.u_mul_e('h', 'weight', 'm'), fn.sum('m', 'h'))\n",
    "            x = graph.ndata.pop('h')\n",
    "            y = torch.cat((y, x), dim=1)\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "def pretreatment(graph, feats):\n",
    "    with graph.local_scope():\n",
    "        row = graph.edges()[0]\n",
    "        col = graph.edges()[1]\n",
    "        edge_index = torch.vstack((row, col))\n",
    "        deg = torch_geometric.utils.degree(col, feats.size(0), dtype=feats.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        edge_weight = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "        \n",
    "    return edge_index, edge_weight\n",
    "\n",
    "\n",
    "class nSGCN(MessagePassing):\n",
    "    def __init__(self, add_self_loops: bool = True, normalize: bool = True):\n",
    "        super(nSGCN, self).__init__()\n",
    "        self.edge_weight = None\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, graph, feats, drop_rate, order):\n",
    "        edge_index, self.edge_weight = pretreatment(graph, feats)\n",
    "        y = self.propagate(edge_index=edge_index, size=None, x=feats, drop_rate=drop_rate)\n",
    "        return y\n",
    "\n",
    "    def message(self, x_j, drop_rate: float):\n",
    "        # normalize\n",
    "        if self.edge_weight is not None:\n",
    "            x_j = x_j * self.edge_weight.view(-1, 1)\n",
    "        if not self.training:\n",
    "            return x_j\n",
    "        # drop messages\n",
    "        x_j = F.dropout(x_j, drop_rate)\n",
    "        return x_j\n",
    "\n",
    "\n",
    "class nSGC(nn.Module):\n",
    "    def __init__(self, G, hid_dim, n_class, K, batchnorm, num_diseases, num_mirnas,\n",
    "                 d_sim_dim, m_sim_dim, out_dim, dropout, slope, node_dropout=0.5, input_droprate=0.0,\n",
    "                 hidden_droprate=0.0):\n",
    "        super(nSGC, self).__init__()\n",
    "        self.G = G\n",
    "        self.hid_dim = hid_dim\n",
    "\n",
    "        self.K = K\n",
    "        self.n_class = n_class\n",
    "        self.num_diseases = num_diseases\n",
    "        self.num_mirnas = num_mirnas\n",
    "        self.disease_nodes = G.filter_nodes(lambda nodes: nodes.data['type'] == 1)\n",
    "        self.mirna_nodes = G.filter_nodes(lambda nodes: nodes.data['type'] == 0)\n",
    "\n",
    "        self.m_fc = nn.Linear(G.ndata['m_sim'].shape[1], hid_dim, bias=False)\n",
    "        self.d_fc = nn.Linear(G.ndata['d_sim'].shape[1], hid_dim, bias=False)\n",
    "        self.f_fc = nn.Linear(out_dim * (K + 1), out_dim)\n",
    "        self.f_fc2 = nn.Linear(out_dim * K, out_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.predict = nn.Linear(out_dim * 2, 1)\n",
    "        self.predict_onlycross = nn.Linear(out_dim, 1)\n",
    "\n",
    "        self.predict_addcross = nn.Linear(out_dim * 3, 1)\n",
    "\n",
    "        self.backbone = nSGCN(True, True)\n",
    "\n",
    "    def forward(self, graph, diseases, mirnas, training=True):\n",
    "        self.G.apply_nodes(lambda nodes: {'z': self.d_fc(nodes.data['d_sim'])}, self.disease_nodes)\n",
    "        self.G.apply_nodes(lambda nodes: {'z': self.m_fc(nodes.data['m_sim'])}, self.mirna_nodes)\n",
    "\n",
    "        feats = self.G.ndata.pop('z')  # 1709*64\n",
    "        X = feats\n",
    "        # X = F.dropout(feats, p=0.5)\n",
    "        if training:\n",
    "            feat0 = []\n",
    "            y = 0 + X\n",
    "            x = self.backbone(graph, X, 0.5, 1)\n",
    "            x = F.relu(x)\n",
    "            y = torch.cat((y, x), dim=1)\n",
    "            for i in range(self.K - 2):\n",
    "                x = self.backbone(graph, x, 0.5, 1)\n",
    "                x = F.relu(x)\n",
    "                y = torch.cat((y, x), dim=1)\n",
    "\n",
    "            h = self.f_fc2(y)\n",
    "            h_diseases = h[diseases]\n",
    "            h_mirnas = h[mirnas]\n",
    "            h_cross = h_diseases * h_mirnas\n",
    "            h_edge = torch.cat((h_diseases, h_mirnas, h_cross), 1)\n",
    "            predict_score = torch.sigmoid(self.predict_addcross(h_edge))\n",
    "            return predict_score\n",
    "\n",
    "        else:\n",
    "            feat0 = nSGCConv(graph, X, self.K)\n",
    "            h = self.f_fc(feat0)\n",
    "            h_diseases = h[diseases]\n",
    "            h_mirnas = h[mirnas]\n",
    "            h_cross = h_diseases * h_mirnas\n",
    "            h_edge = torch.cat((h_diseases, h_mirnas, h_cross), 1)\n",
    "            predict_score = torch.sigmoid(self.predict_addcross(h_edge))\n",
    "            return predict_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3243c5ee-b04c-488e-849d-7a51ebf57634",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = 'FINAL'\n",
    "\n",
    "def load_data(directory, random_seed):\n",
    "    D_SSM1 = np.loadtxt(directory + '/disease_sim/SemSim1.txt')  # 792 * 792\n",
    "    D_SSM2 = np.loadtxt(directory + '/disease_sim/SemSim2.txt')\n",
    "    D_GSM = np.loadtxt(directory + '/diseaseSim/DG.txt')\n",
    "\n",
    "    np.fill_diagonal(D_SSM1, 1)\n",
    "    np.fill_diagonal(D_SSM2, 1)\n",
    "\n",
    "    M_FSM = np.loadtxt(directory + '/miRNA_sim/FuncSim.txt')  # 917 * 917\n",
    "    M_SeSM = np.loadtxt(directory + '/miRNA_sim/SeqSim.txt')  # 917 * 917\n",
    "    M_Fam = np.loadtxt(directory + '/miRNA_sim/Famsim.txt')\n",
    "    M_GSM = np.loadtxt(directory + '/miRNA_sim/MG.txt')\n",
    "\n",
    "    all_associations = pd.read_csv(directory + '/adjacency_matrix.csv',\n",
    "                                   names=['miRNA', 'disease', 'label'])  # 726264 * 3\n",
    "\n",
    "    D_SSM = (D_SSM1 + D_SSM2) / 2\n",
    "    ID = D_SSM\n",
    "    for i in range(D_SSM.shape[0]):\n",
    "        for j in range(D_SSM.shape[1]):\n",
    "            if ID[i][j] == 0:\n",
    "                ID[i][j] = D_GSM[i][j]\n",
    "            else:\n",
    "                ID[i][j] = (D_GSM[i][j] + ID[i][j]) / 2\n",
    "\n",
    "    M_FmSM = M_Fam\n",
    "    for i in range(M_Fam.shape[0]):\n",
    "        for j in range(M_Fam.shape[1]):\n",
    "            if M_Fam[i][j] == 1:\n",
    "                M_FmSM[i][j] = (M_Fam[i][j] + M_GSM[i][j]) / 2\n",
    "            else:\n",
    "                M_FmSM[i][j] = M_GSM[i][j]\n",
    "\n",
    "    #IM = M_SeSM     #1\n",
    "    #IM = M_FSM      #2\n",
    "    #IM = M_FmSM     #3\n",
    "    #IM = np.concatenate((M_SeSM, M_FSM), axis=1) #4\n",
    "    #IM = np.concatenate((M_SeSM, M_FmSM), axis=1) #5\n",
    "    #IM = np.concatenate((M_FSM, M_FmSM), axis=1) #6\n",
    "    IM = np.concatenate((M_SeSM, M_FSM, M_FmSM), axis=1) #7\n",
    "\n",
    "    # M_SSM = (M_FSM + M_SeSM) / 2\n",
    "    #M_SSM = seq*M_SeSM + fun*M_FSM + fam*M_FmSM\n",
    "    # for i in range(M_FSM.shape[0]):\n",
    "    #     for j in range(M_FSM.shape[1]):\n",
    "    #         if IM[i][j] == 0:\n",
    "    #             IM[i][j] = M_GSM[i][j]\n",
    "    #         else:\n",
    "    #             IM[i][j] = (M_GSM[i][j] + IM[i][j]) / 2\n",
    "\n",
    "\n",
    "    known_associations = all_associations.loc[all_associations['label'] == 1]  # 14550 * 3\n",
    "    unknown_associations = all_associations.loc[all_associations['label'] == 0]  # 711714 * 3\n",
    "    random_negative = unknown_associations.sample(n=known_associations.shape[0], random_state=random_seed,\n",
    "                                                  axis=0)  # 14550 * 3\n",
    "    sample_df = known_associations.append(random_negative)\n",
    "    sample_df.reset_index(drop=True, inplace=True)\n",
    "    samples = sample_df.values\n",
    "    return ID, IM, samples\n",
    "\n",
    "def load_data_full(directory, random_seed):\n",
    "    D_SSM1 = np.loadtxt(directory + '/disease_sim/SemSim1.txt')  # 792 * 792\n",
    "    D_SSM2 = np.loadtxt(directory + '/disease_sim/SemSim2.txt')\n",
    "    D_GSM = np.loadtxt(directory + '/diseaseSim/DG.txt')\n",
    "\n",
    "    np.fill_diagonal(D_SSM1, 1)\n",
    "    np.fill_diagonal(D_SSM2, 1)\n",
    "\n",
    "    M_FSM = np.loadtxt(directory + '/miRNA_sim/FuncSim.txt')  # 917 * 917\n",
    "    M_SeSM = np.loadtxt(directory + '/miRNA_sim/SeqSim.txt')  # 917 * 917\n",
    "    M_Fam = np.loadtxt(directory + '/miRNA_sim/Famsim.txt')\n",
    "    M_GSM = np.loadtxt(directory + '/miRNA_sim/MG.txt')\n",
    "\n",
    "    all_associations = pd.read_csv(directory + '/new_adjacency_matrix.csv',\n",
    "                                   names=['miRNA', 'disease', 'label'])  # 726264 * 3\n",
    "\n",
    "    D_SSM = (D_SSM1 + D_SSM2) / 2\n",
    "    ID = D_SSM\n",
    "    for i in range(D_SSM.shape[0]):\n",
    "        for j in range(D_SSM.shape[1]):\n",
    "            if ID[i][j] == 0:\n",
    "                ID[i][j] = D_GSM[i][j]\n",
    "            else:\n",
    "                ID[i][j] = (D_GSM[i][j] + ID[i][j]) / 2\n",
    "\n",
    "    M_FmSM = M_Fam\n",
    "    for i in range(M_Fam.shape[0]):\n",
    "        for j in range(M_Fam.shape[1]):\n",
    "            if M_Fam[i][j] == 1:\n",
    "                M_FmSM[i][j] = (M_Fam[i][j] + M_GSM[i][j]) / 2\n",
    "            else:\n",
    "                M_FmSM[i][j] = M_GSM[i][j]\n",
    "\n",
    "    IM = np.concatenate((M_SeSM, M_FSM, M_FmSM), axis=1) #7\n",
    "\n",
    "    known_associations = all_associations.loc[all_associations['label'] == 1]  # 14550 * 3\n",
    "    unknown_associations = all_associations.loc[all_associations['label'] == 0]  # 711714 * 3\n",
    "    random_negative = unknown_associations.sample(n=known_associations.shape[0], random_state=random_seed,\n",
    "                                                  axis=0)  # 14550 * 3\n",
    "    sample_df = known_associations.append(random_negative)\n",
    "    sample_df.reset_index(drop=True, inplace=True)\n",
    "    samples = all_associations.values\n",
    "    return ID, IM, samples\n",
    "    \n",
    "def build_graph(directory, random_seed):\n",
    "    ID, IM, samples = load_data(directory, random_seed)\n",
    "    g = dgl.DGLGraph()\n",
    "    g.add_nodes(ID.shape[0] + IM.shape[0])\n",
    "    node_type = torch.zeros(g.number_of_nodes(), dtype=torch.int64)\n",
    "    node_type[: ID.shape[0]] = 1\n",
    "    g.ndata['type'] = node_type\n",
    "\n",
    "    d_sim = torch.zeros(g.number_of_nodes(), ID.shape[1])\n",
    "    d_sim[: ID.shape[0], :] = torch.from_numpy(ID.astype('float32'))\n",
    "    g.ndata['d_sim'] = d_sim\n",
    "\n",
    "    m_sim = torch.zeros(g.number_of_nodes(), IM.shape[1])\n",
    "    m_sim[ID.shape[0]: ID.shape[0] + IM.shape[0], :] = torch.from_numpy(IM.astype('float32'))\n",
    "    g.ndata['m_sim'] = m_sim\n",
    "\n",
    "    disease_ids = list(range(1, ID.shape[0] + 1))\n",
    "    mirna_ids = list(range(1, IM.shape[0] + 1))\n",
    "\n",
    "    disease_ids_invmap = {id_: i for i, id_ in enumerate(disease_ids)}\n",
    "    mirna_ids_invmap = {id_: i for i, id_ in enumerate(mirna_ids)}\n",
    "\n",
    "    sample_disease_vertices = [disease_ids_invmap[id_] for id_ in samples[:, 1]]\n",
    "    sample_mirna_vertices = [mirna_ids_invmap[id_] + ID.shape[0] for id_ in samples[:, 0]]\n",
    "\n",
    "    g.add_edges(sample_disease_vertices, sample_mirna_vertices,\n",
    "                data={'label': torch.from_numpy(samples[:, 2].astype('float32'))})\n",
    "    g.add_edges(sample_mirna_vertices, sample_disease_vertices,\n",
    "                data={'label': torch.from_numpy(samples[:, 2].astype('float32'))})\n",
    "\n",
    "    return g, sample_disease_vertices, sample_mirna_vertices, ID, IM, samples\n",
    "\n",
    "\n",
    "def build_graph_full(directory, random_seed):\n",
    "    ID, IM, samples = load_data_full(directory, random_seed)\n",
    "    g = dgl.DGLGraph()\n",
    "    g.add_nodes(ID.shape[0] + IM.shape[0])\n",
    "    node_type = torch.zeros(g.number_of_nodes(), dtype=torch.int64)\n",
    "    node_type[: ID.shape[0]] = 1\n",
    "    g.ndata['type'] = node_type\n",
    "\n",
    "    d_sim = torch.zeros(g.number_of_nodes(), ID.shape[1])\n",
    "    d_sim[: ID.shape[0], :] = torch.from_numpy(ID.astype('float32'))\n",
    "    g.ndata['d_sim'] = d_sim\n",
    "\n",
    "    m_sim = torch.zeros(g.number_of_nodes(), IM.shape[1])\n",
    "    m_sim[ID.shape[0]: ID.shape[0] + IM.shape[0], :] = torch.from_numpy(IM.astype('float32'))\n",
    "    g.ndata['m_sim'] = m_sim\n",
    "\n",
    "    disease_ids = list(range(1, ID.shape[0] + 1))\n",
    "    mirna_ids = list(range(1, IM.shape[0] + 1))\n",
    "\n",
    "    disease_ids_invmap = {id_: i for i, id_ in enumerate(disease_ids)}\n",
    "    mirna_ids_invmap = {id_: i for i, id_ in enumerate(mirna_ids)}\n",
    "\n",
    "    sample_disease_vertices = [disease_ids_invmap[id_] for id_ in samples[:, 1]]\n",
    "    sample_mirna_vertices = [mirna_ids_invmap[id_] + ID.shape[0] for id_ in samples[:, 0]]\n",
    "\n",
    "    g.add_edges(sample_disease_vertices, sample_mirna_vertices,\n",
    "                data={'label': torch.from_numpy(samples[:, 2].astype('float32'))})\n",
    "    g.add_edges(sample_mirna_vertices, sample_disease_vertices,\n",
    "                data={'label': torch.from_numpy(samples[:, 2].astype('float32'))})\n",
    "\n",
    "    return g, sample_disease_vertices, sample_mirna_vertices, ID, IM, samples\n",
    "\n",
    "def weight_reset(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f7ac3622-ee5f-44a9-8d04-1f2f44076f0d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def draw_roc_curve(fprs, tprs, auc_result):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    tprs_interp = []\n",
    "\n",
    "    for i, (fpr, tpr, auc) in enumerate(zip(fprs, tprs, auc_result), 1):\n",
    "        tprs_interp.append(interp(mean_fpr, fpr, tpr))\n",
    "        tprs_interp[-1][0] = 0.0\n",
    "        plt.plot(fpr, tpr, lw=2, alpha=0.3, label=f'ROC Fold {i} (AUC = {auc:.2f})')\n",
    "\n",
    "    mean_tpr = np.mean(tprs_interp, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = metrics.auc(mean_fpr, mean_tpr)\n",
    "    std_tpr = np.std(tprs_interp, axis=0)\n",
    "    \n",
    "    plt.plot(mean_fpr, mean_tpr, color='b', label=f'Mean ROC (AUC = {mean_auc:.2f})', lw=2, alpha=.4)\n",
    "    \n",
    "    std_auc = np.std(auc_result)\n",
    "    plt.fill_between(mean_fpr, mean_tpr - std_tpr, mean_tpr + std_tpr, alpha=.2, color='g', label=f'±1 std. dev. (AUC = {std_auc:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('cv/ROC_curves.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def draw_pr_curve(precisions, recalls, aupr_result):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    mean_recall = np.linspace(0, 1, 1000)\n",
    "    \n",
    "    precisions_interp = []\n",
    "\n",
    "    for i, (precision, recall, aupr) in enumerate(zip(precisions, recalls, aupr_result), 1):\n",
    "        precision = precision[::-1]\n",
    "        recall = recall[::-1]\n",
    "        interp_precision = interp(mean_recall, recall, precision)\n",
    "        precisions_interp.append(interp_precision)\n",
    "        plt.plot(recall, precision, lw=1, alpha=0.3, label=f'PR Fold {i} (AUPR = {aupr:.3f})')\n",
    "\n",
    "    # Calculate mean precision and its standard deviation\n",
    "    mean_precision = np.mean(precisions_interp, axis=0)\n",
    "    std_precision = np.std(precisions_interp, axis=0)\n",
    "    mean_aupr = metrics.auc(mean_recall, mean_precision)\n",
    "    \n",
    "    # Plot mean PR curve\n",
    "    plt.plot(mean_recall, mean_precision, color='b', \n",
    "             label=f'Mean PR (AUPR = {mean_aupr:.3f})', lw=2, alpha=.4)\n",
    "    \n",
    "    # Plot standard deviation area\n",
    "    std_aupr = np.std(aupr_result)\n",
    "    plt.fill_between(mean_recall, mean_precision - std_precision, \n",
    "                     mean_precision + std_precision, alpha=.2, color='g', \n",
    "                     label=f'±1 std. dev. (AUPR std = {std_aupr:.3f})')\n",
    "\n",
    "\n",
    "    plt.xlabel('Recall', fontsize=12)\n",
    "    plt.ylabel('Precision', fontsize=12)\n",
    "    plt.title('Precision-Recall Curve', fontsize=14)\n",
    "    plt.legend(loc=\"lower left\", fontsize=10)\n",
    "    \n",
    "    # Save and show the plot\n",
    "    plt.savefig('cv/PR_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def save_mean_metrics(metrics_dict, param=''):\n",
    "    os.makedirs('cv/metrics/mean', exist_ok=True)\n",
    "    with open(f'cv/metrics/mean/mean_metrics{param}.txt', 'w') as f:\n",
    "        for key, value in metrics_dict.items():\n",
    "            f.write(f\"{key} mean: {value['mean']:.4f}, variance: {value['var']:.4f}\\n\")\n",
    "\n",
    "def save_loss_plot(train_losses, val_losses, fold=''):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(len(train_losses)), train_losses, label='Training Loss')\n",
    "    plt.plot(range(len(val_losses)), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Training and Validation Loss {fold}')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    os.makedirs('cv/loss', exist_ok=True)\n",
    "    plt.savefig(f'cv/loss/training_validation_loss{fold}.png')\n",
    "    plt.close()\n",
    "\n",
    "def save_roc_curve(fpr, tpr, auc, fold=''):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', label='Random Classifier')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'Receiver Operating Characteristic (ROC) Curve {fold}')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "    os.makedirs('cv/roc', exist_ok=True)\n",
    "    plt.savefig(f'cv/roc/roc_curve{fold}.png')\n",
    "    plt.close()\n",
    "\n",
    "def save_pr_curve(recall, precision, aupr, fold=''):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(recall, precision, label=f'PR Curve (AUPR = {aupr:.4f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'Precision-Recall Curve {fold}')\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.grid(True)\n",
    "    os.makedirs('cv/pr', exist_ok=True)\n",
    "    plt.savefig(f'cv/pr/pr_curve{fold}.png')\n",
    "    plt.close()\n",
    "\n",
    "def save_accuracy_curve(train_accs, val_accs, fold=''):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(len(train_accs)), train_accs, label='Training Accuracy')\n",
    "    plt.plot(range(len(val_accs)), val_accs, label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'Training and Validation Accuracy {fold}')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "    os.makedirs('cv/acc', exist_ok=True)\n",
    "    plt.savefig(f'cv/acc/accuracy_curve{fold}.png')\n",
    "    plt.close()\n",
    "\n",
    "def save_auc_curve(train_aucs, val_aucs, fold=''):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(len(train_aucs)), train_aucs, label='Training AUC')\n",
    "    plt.plot(range(len(val_aucs)), val_aucs, label='Validation AUC')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.title(f'Training and Validation AUC {fold}')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "    os.makedirs('cv/auc', exist_ok=True)\n",
    "    plt.savefig(f'cv/auc/auc_curve{fold}.png')\n",
    "    plt.close()\n",
    "\n",
    "def save_metrics(metrics_dict, fold):\n",
    "    os.makedirs('cv/metrics', exist_ok=True)\n",
    "    with open(f'cv/metrics/fold{fold}_metrics.txt', 'w') as f:\n",
    "        for key, value in metrics_dict.items():\n",
    "            f.write(f\"{key}: {value:.4f}\\n\")\n",
    "\n",
    "def calculate_metrics(label, score):\n",
    "    pred = [0 if j < 0.5 else 1 for j in score]\n",
    "\n",
    "    auc = metrics.roc_auc_score(label, score)\n",
    "    acc = metrics.accuracy_score(label, pred)    \n",
    "    pre = metrics.precision_score(label, pred)\n",
    "    rec = metrics.recall_score(label, pred)\n",
    "    f1 = metrics.f1_score(label, pred)\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(label, score)\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(label, pred).ravel()\n",
    "    mcc = metrics.matthews_corrcoef(label, pred)\n",
    "    test_auc = metrics.auc(fpr, tpr)\n",
    "    spe = tn / (tn + fp)\n",
    "    precision, recall, _ = metrics.precision_recall_curve(label, score)\n",
    "    aupr = metrics.auc(recall, precision)\n",
    "\n",
    "    return acc, pre, rec, spe, f1, auc, aupr, mcc, fpr, tpr, precision, recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8c82e895-d363-4d5e-8255-6e5edd52d5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def Train(directory, epochs, n_classes, in_size, out_dim, dropout, slope, lr, wd, random_seed, cuda, kk):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "\n",
    "    context = torch.device('cpu')\n",
    "\n",
    "    g, disease_vertices, mirna_vertices, ID, IM, samples = build_graph(directory, random_seed)\n",
    "    samples_df = pd.DataFrame(samples, columns=['miRNA', 'disease', 'label'])\n",
    "    g.to(context)\n",
    "\n",
    "    auc_result = []\n",
    "    acc_result = []\n",
    "    pre_result = []\n",
    "    recall_result = []\n",
    "    f1_result = []\n",
    "    aupr_result = []\n",
    "    mcc_result = []\n",
    "    specificity_result = []\n",
    "\n",
    "    fprs = []\n",
    "    tprs = []\n",
    "    pres = []\n",
    "    recs = []\n",
    "\n",
    "    i = 0\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=random_seed)\n",
    "\n",
    "    for train_idx, test_idx in kf.split(samples[:, 2]):\n",
    "        train_losses, train_accs, val_losses, val_accs = [], [], [], []\n",
    "        train_aucs, val_aucs = [], []\n",
    "        i += 1\n",
    "        print('\\nTraining for Fold', i)\n",
    "\n",
    "        samples_df['train'] = 0\n",
    "        samples_df['train'].iloc[train_idx] = 1\n",
    "\n",
    "        train_tensor = torch.from_numpy(samples_df['train'].values.astype('int64'))\n",
    "\n",
    "        edge_data = {'train': train_tensor}\n",
    "\n",
    "        g.edges[disease_vertices, mirna_vertices].data.update(edge_data)\n",
    "        g.edges[mirna_vertices, disease_vertices].data.update(edge_data)\n",
    "\n",
    "        train_eid = g.filter_edges(lambda edges: edges.data['train'])\n",
    "        g_train = g.edge_subgraph(train_eid, relabel_nodes=False)\n",
    "        label_train = g_train.edata['label'].unsqueeze(1)\n",
    "        src_train, dst_train = g_train.all_edges()\n",
    "        test_eid = g.filter_edges(lambda edges: edges.data['train'] == 0)\n",
    "        src_test, dst_test = g.find_edges(test_eid)\n",
    "        label_test = g.edges[test_eid].data['label'].unsqueeze(1)\n",
    "        print('Training edges:', len(train_eid))\n",
    "        print('Testing edges:', len(test_eid))\n",
    "\n",
    "        model = nSGC(G=g_train,\n",
    "                     hid_dim=in_size,\n",
    "                     n_class=n_classes,\n",
    "                     K=kk,\n",
    "                     batchnorm=False,\n",
    "                     num_diseases=ID.shape[0],\n",
    "                     num_mirnas=IM.shape[0],\n",
    "                     d_sim_dim=ID.shape[1],\n",
    "                     m_sim_dim=IM.shape[1],\n",
    "                     out_dim=out_dim,\n",
    "                     dropout=dropout,\n",
    "                     slope=slope)\n",
    "        \n",
    "        model.apply(weight_reset)\n",
    "        model.to(context)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "        loss = nn.BCELoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            start = time.time()\n",
    "\n",
    "            model.train()\n",
    "            with torch.autograd.set_detect_anomaly(True):\n",
    "                score_train = model(g_train, src_train, dst_train, True)\n",
    "                loss_train = loss(score_train, label_train)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss_train.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                score_val = model(g, src_test, dst_test, True)\n",
    "                loss_val = loss(score_val, label_test)\n",
    "\n",
    "            score_train_cpu = np.squeeze(score_train.cpu().detach().numpy())\n",
    "            score_val_cpu = np.squeeze(score_val.cpu().detach().numpy())\n",
    "            label_train_cpu = np.squeeze(label_train.cpu().detach().numpy())\n",
    "            label_val_cpu = np.squeeze(label_test.cpu().detach().numpy())\n",
    "\n",
    "            pred_train = [0 if j < 0.5 else 1 for j in score_train_cpu]\n",
    "            pred_val = [0 if j < 0.5 else 1 for j in score_val_cpu]\n",
    "        \n",
    "            train_acc = metrics.accuracy_score(label_train_cpu, pred_train) \n",
    "            val_acc = metrics.accuracy_score(label_val_cpu, pred_val) \n",
    "            train_auc = metrics.roc_auc_score(label_train_cpu, score_train_cpu)\n",
    "            val_auc = metrics.roc_auc_score(label_val_cpu, score_val_cpu)\n",
    "\n",
    "            train_losses.append(loss_train.item())\n",
    "            val_losses.append(loss_val.item())\n",
    "            train_accs.append(train_acc)\n",
    "            val_accs.append(val_acc)\n",
    "            train_aucs.append(train_auc)\n",
    "            val_aucs.append(val_auc)\n",
    "\n",
    "            end = time.time()\n",
    "            if (epoch + 1) % 50 == 0:\n",
    "                print('    Epoch:', epoch + 1, 'Train Loss: %.4f' % loss_train.item(), 'Val Loss: %.4f' % loss_val.item(), 'Time: %.2f' % (end - start))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            score_test = model(g, src_test, dst_test, True)\n",
    "\n",
    "        score_test_cpu = np.squeeze(score_test.cpu().detach().numpy())\n",
    "        label_test_cpu = np.squeeze(label_test.cpu().detach().numpy())\n",
    "\n",
    "        test_acc, test_pre, test_rec, test_spe, test_f1, test_auc, test_aupr, test_mcc, fpr, tpr, precision, recall = calculate_metrics(label_test_cpu, score_test_cpu)\n",
    "\n",
    "\n",
    "        save_loss_plot(train_losses, val_losses, f'Fold{i}')\n",
    "        save_accuracy_curve(train_accs, val_accs, f'Fold{i}')\n",
    "        save_auc_curve(train_aucs, val_aucs, f'Fold{i}')\n",
    "        save_roc_curve(fpr, tpr, test_auc, f'Fold{i}')\n",
    "        save_pr_curve(recall, precision, test_aupr, f'Fold{i}')\n",
    "    \n",
    "\n",
    "        print('Fold:', i, 'Acc: %.4f' % test_acc, 'Pre: %.4f' % test_pre, 'Rec: %.4f' % test_rec, 'Spec: %.4f' % test_spe,\n",
    "              'F1: %.4f' % test_f1, 'AUC: %.4f' % test_auc, 'AUPR: %.4f' % test_aupr, 'MCC: %.4f' % test_mcc)\n",
    "                      \n",
    "        # Save metrics for this fold\n",
    "        fold_metrics = {\n",
    "            'Accuracy': test_acc,\n",
    "            'Precision': test_pre,\n",
    "            'Recall': test_rec,\n",
    "            'F1-score': test_f1,\n",
    "            'AUC': test_auc,\n",
    "            'AUPR': test_aupr,\n",
    "            'MCC': test_mcc,\n",
    "            'Specificity': test_spe\n",
    "        }\n",
    "        save_metrics(fold_metrics, i)\n",
    "\n",
    "        auc_result.append(test_auc)\n",
    "        acc_result.append(test_acc)\n",
    "        pre_result.append(test_pre)\n",
    "        recall_result.append(test_rec)\n",
    "        f1_result.append(test_f1)\n",
    "        aupr_result.append(test_aupr)\n",
    "        mcc_result.append(test_mcc)\n",
    "        specificity_result.append(test_spe)\n",
    "\n",
    "        fprs.append(fpr)\n",
    "        tprs.append(tpr)\n",
    "        pres.append(precision)\n",
    "        recs.append(recall)\n",
    "\n",
    "    # Draw ROC and PR curves\n",
    "    draw_roc_curve(fprs, tprs, auc_result)\n",
    "    draw_pr_curve(pres, recs, aupr_result)\n",
    "\n",
    "    # Calculate and save mean metrics\n",
    "    mean_metrics = {\n",
    "        'AUC': {'mean': np.mean(auc_result), 'var': np.std(auc_result)},\n",
    "        'Accuracy': {'mean': np.mean(acc_result), 'var': np.std(acc_result)},\n",
    "        'Precision': {'mean': np.mean(pre_result), 'var': np.std(pre_result)},\n",
    "        'Recall': {'mean': np.mean(recall_result), 'var': np.std(recall_result)},\n",
    "        'F1-score': {'mean': np.mean(f1_result), 'var': np.std(f1_result)},\n",
    "        'AUPR': {'mean': np.mean(aupr_result), 'var': np.std(aupr_result)},\n",
    "        'MCC': {'mean': np.mean(mcc_result), 'var': np.std(mcc_result)},\n",
    "        'Specificity': {'mean': np.mean(specificity_result), 'var': np.std(specificity_result)}\n",
    "    }\n",
    "    save_mean_metrics(mean_metrics, parameter)\n",
    "\n",
    "    print('DONE!')\n",
    "    print('-----------------------------------------------------------------------------------------------')\n",
    "    for metric, values in mean_metrics.items():\n",
    "        print(f'{metric} mean: {values[\"mean\"]:.4f}, variance: {values[\"var\"]:.4f}')\n",
    "    \n",
    "    torch.save(model.state_dict(), f'best_model.pth')\n",
    "    return fprs, tprs, auc_result, pres, recs, aupr_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f17f984c-f082-405b-9298-71b9e08d1372",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fprs, tprs, auc, precisions, recalls, aupr = Train(directory='data',\n",
    "                                                  epochs=200,\n",
    "                                                  n_classes=64,\n",
    "                                                  in_size=64,\n",
    "                                                  out_dim=64,\n",
    "                                                  dropout=0.5,\n",
    "                                                  slope=0.2,\n",
    "                                                  lr=0.0001,\n",
    "                                                  wd=5e-3,\n",
    "                                                  random_seed=1225,\n",
    "                                                  cuda=True, kk=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "3a5638d8-c081-431c-b74b-1cc9a60418cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_full_data(directory):\n",
    "    g, disease_vertices, mirna_vertices, ID, IM, samples = build_graph_full(directory, random_seed=42)\n",
    "    return g, disease_vertices, mirna_vertices, ID, IM, samples\n",
    "\n",
    "def train_full_model(g, disease_vertices, mirna_vertices, ID, IM, samples):\n",
    "    epochs = 200\n",
    "    lr = 0.0001\n",
    "    wd = 5e-3\n",
    "\n",
    "    model = nSGC(G=g,\n",
    "                 hid_dim=64,\n",
    "                 n_class=64,\n",
    "                 K=4,\n",
    "                 batchnorm=False,\n",
    "                 num_diseases=ID.shape[0],\n",
    "                 num_mirnas=IM.shape[0],\n",
    "                 d_sim_dim=ID.shape[1],\n",
    "                 m_sim_dim=IM.shape[1],\n",
    "                 out_dim=64,\n",
    "                 dropout=0.5,\n",
    "                 slope=0.2)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    loss_fn = nn.BCELoss()\n",
    "\n",
    "    src, dst = g.all_edges()\n",
    "    label = g.edata['label'].unsqueeze(1)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        score = model(g, src, dst, True)\n",
    "        loss = loss_fn(score, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f'Epoch: {epoch + 1}, Loss: {loss.item():.4f}, Time: {end - start:.2f}s')\n",
    "    \n",
    "    torch.save(model.state_dict(), './GCN-MDA')    \n",
    "    return model\n",
    "\n",
    "def predict_associations(model, g, node_id):\n",
    "    g_sub = g.in_subgraph(node_id, relabel_nodes=False, store_ids=True)\n",
    "    src, dst = g_sub.all_edges()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        scores = model(g, src, dst, True)\n",
    "    \n",
    "    return scores, g_sub\n",
    "\n",
    "def predict_associations2(model, g, node_id):\n",
    "    g_sub = g.in_subgraph(node_id, relabel_nodes=False, store_ids=True)\n",
    "    src, dst = g_sub.all_edges()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        scores = model(g, src, dst, True)\n",
    "    \n",
    "    return scores.squeeze().tolist()\n",
    "\n",
    "def predict_associations3(model, g, ind_d, mir):\n",
    "    dst = torch.zeros(mir, dtype=torch.int64).tolist()\n",
    "    dst[ind_d] = 1\n",
    "    \n",
    "    # g_sub = g.in_subgraph(node_id, relabel_nodes=False, store_ids=True)\n",
    "    # src, dst = g_sub.all_edges()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        scores = model(g, dst, mir, True)\n",
    "    \n",
    "    return scores.squeeze().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "558e6a97-d3ac-4dd3-8c77-5a463782d2cc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# g, disease_vertices, mirna_vertices, ID, IM, samples = load_full_data('data/')\n",
    "# model = nSGC(G=g,\n",
    "#              hid_dim=64,           # Use the same parameters as used during training\n",
    "#              n_class=64,\n",
    "#              K=4,\n",
    "#              batchnorm=False,\n",
    "#              num_diseases=ID.shape[0],\n",
    "#              num_mirnas=IM.shape[0],\n",
    "#              d_sim_dim=ID.shape[1],\n",
    "#              m_sim_dim=IM.shape[1],\n",
    "#              out_dim=64,\n",
    "#              dropout=0.5,\n",
    "#              slope=0.2)\n",
    "# model = model.load_state_dict(torch.load('./GCN-MDA'))\n",
    "# #disease_nodes = g.filter_nodes(lambda nodes: nodes.data['type'] == 0)\n",
    "# predictions = predict_associations(model, 0)\n",
    "# predictions = predictions.squeeze().tolist()\n",
    "# #len(predictions.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "4245e08c-44bd-4eeb-bc26-cdbcbd4f7f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "g, disease_vertices, mirna_vertices, ID, IM, samples = load_full_data('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46000583-5bad-4786-9763-200699d174fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_full_model(g, disease_vertices, mirna_vertices, ID, IM, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c83faf-8c32-4920-8649-1103f56fe86e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K=50\n",
    "\n",
    "disease_df = pd.read_csv(\"data/disease_name.csv\", names=['disease_name'])\n",
    "mirna_df = pd.read_csv(\"data/miRNA_name.csv\", names=['mirna_name'])\n",
    "output_dir = 'disease_predictions'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "diseases_to_predict = ['breast neoplasms', \n",
    "                       'leukemia', 'lung neoplasms']\n",
    "\n",
    "for disease_name in diseases_to_predict:\n",
    "    print(f\"\\nPredicting top {K} miRNAs for {disease_name}:\")\n",
    "    disease_id = disease_df.index[disease_df['disease_name'] == disease_name].tolist()\n",
    "    print(disease_id)\n",
    "    predictions, gg = predict_associations(model, g, disease_id)\n",
    "    predictions = predictions.squeeze().tolist()\n",
    "\n",
    "    a, b = gg.all_edges()\n",
    "    print(a.shape, b.shape, len(predictions))\n",
    "    a = a.tolist()\n",
    "    b = b.tolist()\n",
    "    df = pd.DataFrame({\n",
    "        'src': a,\n",
    "        'dst': b,\n",
    "        'preds': predictions\n",
    "    })\n",
    "    \n",
    "    filtered_df = df[(df['src'] == disease_id[0]) | (df['dst'] == disease_id[0])]\n",
    "    sorted_df = filtered_df.sort_values(by='preds', ascending=False)\n",
    "    bib = sorted_df[:K]\n",
    "    preddss = bib['preds'].values\n",
    "    selected_columns = bib[['dst', 'src']]\n",
    "    values_list = selected_columns.values.flatten().tolist()\n",
    "    top_k_indices = []\n",
    "    for x in values_list:\n",
    "        if x != disease_id[0]:\n",
    "            top_k_indices.append(x)\n",
    "        \n",
    "    # Create a list to store top K results for this disease\n",
    "    top_k_results = []\n",
    "    \n",
    "    for rank, idx in enumerate(top_k_indices, 1):\n",
    "        mirna_name = mirna_df['mirna_name'].iloc[idx - disease_df.shape[0]]\n",
    "        score = preddss[rank - 1]\n",
    "        top_k_results.append({\n",
    "            'miRNA': mirna_name,\n",
    "            'Rank': rank,\n",
    "            'Score': score\n",
    "        })\n",
    "        print(f\"{rank}. {mirna_name}: {score:.4f}\")\n",
    "    \n",
    "    # Save top K results for this disease to a CSV file\n",
    "    disease_filename = disease_name.replace(\" \", \"_\").lower()\n",
    "    output_file = os.path.join(output_dir, f'{disease_filename}_top_{K}_predictions2.csv')\n",
    "    fieldnames = ['miRNA', 'Score', 'Rank']\n",
    "\n",
    "    with open(output_file, 'w', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for result in top_k_results:\n",
    "            writer.writerow(result)\n",
    "    \n",
    "    print(f\"Top {K} prediction results for {disease_name} have been saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
